{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d380bafa-df57-4089-9b2a-5771c483b3ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\mar4u\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Import statements and paths\n",
    "\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForTokenClassification\n",
    ")\n",
    "from datasets import Dataset\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import numpy as np\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Define paths based on your structure\n",
    "BASE_PATH = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "TRAIN_FILE = os.path.join(BASE_PATH, 'dataset', 'ner_training_data', 'train.json')\n",
    "VAL_FILE = os.path.join(BASE_PATH, 'dataset', 'ner_training_data', 'val.json')\n",
    "OUTPUT_DIR = os.path.join(BASE_PATH, 'models', 'ner_model')\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "98d5b75b-ebd4-40bd-8833-244475501cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second cell - Complete NERTrainer class with all methods\n",
    "class NERTrainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name=\"bert-base-uncased\",\n",
    "        max_length=128,\n",
    "        train_batch_size=16,\n",
    "        eval_batch_size=16,\n",
    "        learning_rate=5e-5,\n",
    "        num_epochs=5\n",
    "    ):\n",
    "        self.model_name = model_name\n",
    "        self.max_length = max_length\n",
    "        self.train_batch_size = train_batch_size\n",
    "        self.eval_batch_size = eval_batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_epochs = num_epochs\n",
    "        \n",
    "        # Define label mappings\n",
    "        self.label2id = {\"O\": 0, \"ANIMAL\": 1}\n",
    "        self.id2label = {0: \"O\", 1: \"ANIMAL\"}\n",
    "        \n",
    "        # Initialize tokenizer and model\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForTokenClassification.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=len(self.label2id),\n",
    "            id2label=self.id2label,\n",
    "            label2id=self.label2id\n",
    "        )\n",
    "        \n",
    "        self.data_collator = DataCollatorForTokenClassification(\n",
    "            self.tokenizer,\n",
    "            pad_to_multiple_of=8\n",
    "        )\n",
    "\n",
    "    def load_data(self, train_file: str, val_file: str):\n",
    "        \"\"\"Load and preprocess the data\"\"\"\n",
    "        def process_file(file_path):\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            processed_data = []\n",
    "            for item in data:\n",
    "                try:\n",
    "                    processed = self.prepare_example(item[\"sentence\"], item[\"entities\"])\n",
    "                    processed_data.append(processed)\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Error processing example: {str(e)}\")\n",
    "                    continue\n",
    "            \n",
    "            return Dataset.from_dict({\n",
    "                \"tokens\": [x[\"tokens\"] for x in processed_data],\n",
    "                \"labels\": [x[\"labels\"] for x in processed_data]\n",
    "            })\n",
    "        \n",
    "        train_dataset = process_file(train_file)\n",
    "        val_dataset = process_file(val_file)\n",
    "        \n",
    "        logger.info(f\"Loaded {len(train_dataset)} training examples\")\n",
    "        logger.info(f\"Loaded {len(val_dataset)} validation examples\")\n",
    "        \n",
    "        return train_dataset, val_dataset\n",
    "\n",
    "    def prepare_example(self, text: str, entities: list):\n",
    "        \"\"\"Prepare a single example with accurate token labeling\"\"\"\n",
    "        words = text.split()\n",
    "        labels = [\"O\"] * len(words)\n",
    "        \n",
    "        for start, end, label in entities:\n",
    "            entity_positions = self.get_entity_positions(text, start, end)\n",
    "            for pos in entity_positions:\n",
    "                if pos < len(labels):\n",
    "                    labels[pos] = \"ANIMAL\"\n",
    "        \n",
    "        return {\n",
    "            \"tokens\": words,\n",
    "            \"labels\": [self.label2id[label] for label in labels]\n",
    "        }\n",
    "\n",
    "    def get_entity_positions(self, text: str, start: int, end: int):\n",
    "        \"\"\"Get token positions for an entity\"\"\"\n",
    "        words = text.split()\n",
    "        char_count = 0\n",
    "        entity_tokens = []\n",
    "        \n",
    "        for i, word in enumerate(words):\n",
    "            word_start = char_count\n",
    "            word_end = char_count + len(word)\n",
    "            \n",
    "            if i > 0:  # Add space after first word\n",
    "                word_start += 1\n",
    "                word_end += 1\n",
    "                char_count += 1\n",
    "            \n",
    "            if word_end > start and word_start < end:\n",
    "                entity_tokens.append(i)\n",
    "            \n",
    "            char_count += len(word)\n",
    "        \n",
    "        return entity_tokens\n",
    "\n",
    "    def tokenize_and_align_labels(self, examples):\n",
    "        \"\"\"Tokenize and align labels with tokens\"\"\"\n",
    "        tokenized_inputs = self.tokenizer(\n",
    "            examples[\"tokens\"],\n",
    "            truncation=True,\n",
    "            is_split_into_words=True,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\"\n",
    "        )\n",
    "\n",
    "        labels = []\n",
    "        for i, label in enumerate(examples[\"labels\"]):\n",
    "            word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "            previous_word_idx = None\n",
    "            label_ids = []\n",
    "\n",
    "            for word_idx in word_ids:\n",
    "                if word_idx is None:\n",
    "                    label_ids.append(-100)\n",
    "                elif word_idx != previous_word_idx:\n",
    "                    label_ids.append(label[word_idx])\n",
    "                else:\n",
    "                    label_ids.append(-100)\n",
    "                previous_word_idx = word_idx\n",
    "\n",
    "            labels.append(label_ids)\n",
    "\n",
    "        tokenized_inputs[\"labels\"] = labels\n",
    "        return tokenized_inputs\n",
    "\n",
    "    def compute_metrics(self, p):\n",
    "        \"\"\"Compute metrics for evaluation\"\"\"\n",
    "        predictions = np.argmax(p.predictions, axis=2)\n",
    "        \n",
    "        true_labels = [[l for l in label if l != -100] for label in p.label_ids]\n",
    "        true_predictions = [\n",
    "            [p for (p, l) in zip(pred, gold_label) if l != -100]\n",
    "            for pred, gold_label in zip(predictions, p.label_ids)\n",
    "        ]\n",
    "\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "            [l for labels in true_labels for l in labels],\n",
    "            [p for preds in true_predictions for p in preds],\n",
    "            average='binary',\n",
    "            zero_division=0\n",
    "        )\n",
    "        \n",
    "        accuracy = accuracy_score(\n",
    "            [l for labels in true_labels for l in labels],\n",
    "            [p for preds in true_predictions for p in preds]\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'accuracy': accuracy,\n",
    "            'f1': f1,\n",
    "            'precision': precision,\n",
    "            'recall': recall\n",
    "        }\n",
    "\n",
    "    def train(self, train_dataset, val_dataset, output_dir: str):\n",
    "        \"\"\"Train the model\"\"\"\n",
    "        # Process datasets\n",
    "        tokenized_train = train_dataset.map(\n",
    "            self.tokenize_and_align_labels,\n",
    "            batched=True,\n",
    "            remove_columns=train_dataset.column_names\n",
    "        )\n",
    "        tokenized_val = val_dataset.map(\n",
    "            self.tokenize_and_align_labels,\n",
    "            batched=True,\n",
    "            remove_columns=val_dataset.column_names\n",
    "        )\n",
    "\n",
    "        # Define training arguments\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            learning_rate=self.learning_rate,\n",
    "            per_device_train_batch_size=self.train_batch_size,\n",
    "            per_device_eval_batch_size=self.eval_batch_size,\n",
    "            num_train_epochs=self.num_epochs,\n",
    "            weight_decay=0.01,\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"f1\",\n",
    "            logging_dir=os.path.join(output_dir, \"logs\"),\n",
    "            logging_steps=10\n",
    "        )\n",
    "\n",
    "        # Initialize trainer\n",
    "        trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=tokenized_train,\n",
    "            eval_dataset=tokenized_val,\n",
    "            data_collator=self.data_collator,\n",
    "            compute_metrics=self.compute_metrics\n",
    "        )\n",
    "\n",
    "        # Train model\n",
    "        logger.info(\"Starting training...\")\n",
    "        trainer.train()\n",
    "        \n",
    "        # Save final model\n",
    "        model_save_path = os.path.join(output_dir, \"final\")\n",
    "        self.model.save_pretrained(model_save_path)\n",
    "        self.tokenizer.save_pretrained(model_save_path)\n",
    "        logger.info(f\"Model saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8bf2d972-9ae5-4e32-8d52-4ae9c7816f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "INFO:__main__:Loaded 444 training examples\n",
      "INFO:__main__:Loaded 111 validation examples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd5cc682a4c64452b604eab0b5defad0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/444 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38b5d66b42ed400ab2697adb6d02e2b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/111 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mar4u\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "INFO:__main__:Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='140' max='140' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [140/140 14:48, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.006400</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Model saved to C:\\Users\\mar4u\\Documents\\DS-Test-2025\\task2\\models\\ner_model\\final\n"
     ]
    }
   ],
   "source": [
    "# Initialize and train the model\n",
    "trainer = NERTrainer()\n",
    "train_dataset, val_dataset = trainer.load_data(TRAIN_FILE, VAL_FILE)\n",
    "trainer.train(train_dataset, val_dataset, OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc150d8-2d97-4d27-87dd-0eff6d6cca6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
